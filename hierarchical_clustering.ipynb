{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82a13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from random import choices, sample\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1081ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_cluster(X, max_clusters=None):\n",
    "    \"\"\"\n",
    "    Builds a hierarchical clustering using the linkage function from scipy \n",
    "    (agglomerative clustering using ward's linkage and euclidean distance).\n",
    "    Records clusters as they merge; output clusters start at the root node.\n",
    "    The two child nodes, associated with each new node, are recorded for reference.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [N x d] matrix, containing N observations of d-dimensional input data\n",
    "        max_clusters (int, optional): Determines the maximum number of clusters defined. \n",
    "            Default value (N-1) considers clusters down to single observations.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing N-1 arrays, each represents a node in the hierarchical tree,\n",
    "            containing the data indices associated with each cluster.\n",
    "        np.ndarray: A [2 x N-1] matrix, containing the 2 child nodes for each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    if max_clusters is None:\n",
    "        max_clusters = len(X) - 1\n",
    "\n",
    "    # Build hierarchical tree\n",
    "    Z = linkage(X, method='ward', metric='euclidean')\n",
    "\n",
    "    N = len(X)\n",
    "    u = [None] * (N - 1)\n",
    "    ch = np.zeros((2, N - 1), dtype=int)\n",
    "\n",
    "    # Using the output of linkage (Z) to build clusters\n",
    "    for m in range(len(u)):\n",
    "        # Join parts p1 & p2 of the clustering together using Z info\n",
    "        if Z[m, 0] < N:\n",
    "            p1 = [Z[m, 0]]\n",
    "            c1 = int(p1[0])\n",
    "        else:\n",
    "            p1 = u[int(Z[m, 0] - N)]\n",
    "            c1 = 2*N - int(Z[m, 0]) - 1\n",
    "\n",
    "        if Z[m, 1] < N:\n",
    "            p2 = [Z[m, 1]]\n",
    "            c2 = int(p2[0])\n",
    "        else:\n",
    "            p2 = u[int(Z[m, 1] - N)]\n",
    "            c2 = 2*N - int(Z[m, 1]) - 1\n",
    "     \n",
    "        u[m] = p1 + p2\n",
    "        ch[:, m] = [c1, c2]\n",
    "\n",
    "    u = list(reversed(u))\n",
    "    u = u[:max_clusters]\n",
    "    u_= [[int(x) for x in sublist] for sublist in u]\n",
    "    u = [x for x in u_ if x]\n",
    "\n",
    "    ch = np.fliplr(ch[:, :max_clusters])\n",
    "    ch = ch.astype(int)\n",
    "    \n",
    "    return u, ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5de25ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def prune_label(P, u, Au, Lu, ch, z, N, eu, wu):\n",
    "    \"\"\"\n",
    "    Function to refine the pruning (P) and labelling (L) for the DH active learner.\n",
    "    \n",
    "    Parameters:\n",
    "    - P : list\n",
    "        Input pruning, to be refined (array of node/cluster numbers).\n",
    "    - u : dict\n",
    "        Contains N-1 arrays. Each array represents a node in the hierarchical tree.\n",
    "    - Au : list\n",
    "        Logistic array, indicating the admissibility of each cluster in the current pruning.\n",
    "    - Lu : dict\n",
    "        Cell containing the majority label(s), for each node.\n",
    "    - ch : numpy array\n",
    "        2x(N-1) matrix containing the 2 children nodes for each cluster.\n",
    "    - z : numpy array\n",
    "        Sampled data information array.\n",
    "    - N : int\n",
    "        Total number of clusters.\n",
    "    - eu : list\n",
    "        Error associated with propagating the majority label to unlabelled instances.\n",
    "    - wu : list\n",
    "        Node weights - proportion of the total data in each node.\n",
    "        \n",
    "    Returns:\n",
    "    - P_ : list\n",
    "        Output (refined) pruning.\n",
    "    - L : list\n",
    "        The majority label for each cluster in the refined pruning.\n",
    "    - XL : numpy array\n",
    "        The labelled dataset provided by the DH learner.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = [0] * len(u)  # Initialize admissible cluster label pairs\n",
    "    #print(\"type of P el is \", type(P[0]))\n",
    "    print(\"value of Au is \")\n",
    "    print(np.sum(Au))\n",
    "    print(\"***\")\n",
    "    P_ = deepcopy(P)  # Define working pruning\n",
    "\n",
    "    for i, v in enumerate(P):  # For each node in the current pruning\n",
    "        # LABEL parent node in case descendants are not admissible\n",
    "        Lu[0] = 0  # Arbitrary label of root\n",
    "        if len(P) != 1 and len(Lu[v]) == 1:\n",
    "            L[v] = int(Lu[v])\n",
    "\n",
    "        # Identify first descendants...\n",
    "        chv = ch[:, v]\n",
    "        Pv = [deepcopy(v)]\n",
    "        Achv = [int(Au[x]) for x in chv] # x was previously x - 1 but we already decrement indices by 1 in h_cluster\n",
    "\n",
    "        # While at least one pair of siblings is admissible, refine and label Pv\n",
    "        while np.sum(np.sum(Achv, axis = 0).astype(int) == 2) >= 1:\n",
    "            print(\"inside the while loop \", len(Achv), \" type is \", type(Achv[0]))\n",
    "            i_ch = np.where(np.sum(Achv, axis = 0).astype(int) == 2)[0] # find indices of columns that sum to 2\n",
    "\n",
    "            for ich in i_ch:\n",
    "                ep = eu[Pv[ich]] # error parent\n",
    "                ech = (1 / np.sum([wu[x] for x in chv[:, ich]])) * np.sum(       \n",
    "                    [wu[x] * eu[x] for x in chv[:, ich]])\n",
    "                Lch = [int(Lu[x]) for x in chv[:, ich]]\n",
    "                Lch_log = [len(x) == 1 for x in Lch]\n",
    "\n",
    "                if len(Lu[Pv[ich]]) == 1 and ech < ep and np.sum(Lch_log) == 2:\n",
    "                    Pv[ich] = 0\n",
    "                    u_ = chv[:, ich]\n",
    "                    Pv.extend(deepcopy(u_)) # not sure if I should just leave deepcopy(u_) this way or [deepcopy(u_)]\n",
    "\n",
    "            Pv = [x for x in Pv if x != 0] # remove zero nodes (replaced parent clusters)\n",
    "\n",
    "            if len(set(Pv)) > chv.shape[1]:\n",
    "                chv = ch[:, Pv]\n",
    "                Achv = [int(Au[x]) for x in chv]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if len(Pv) > 1:\n",
    "            print(f\"\\nTOTAL CLUSTERS {len(P_)}: nodes {Pv} replace node [{v}]\")\n",
    "            P_ = [x for x in P_ if x != v]\n",
    "            P_.extend(Pv)\n",
    "            for uw in Pv:\n",
    "                L[uw] = Lu[uw]\n",
    "\n",
    "    xl = [0] * N\n",
    "    for idx, v in enumerate(P):\n",
    "        if len(P) >= len(set(z[:, -1])):\n",
    "            xi = u[v]\n",
    "            xl[xi - 1] = L[v - 1]\n",
    "\n",
    "    for idx in range(z.shape[0]):\n",
    "        z_ = int(z[idx, 0])\n",
    "        zl = z[idx, -1]\n",
    "        xl[z_ - 1] = int(zl)\n",
    "\n",
    "    XL = np.array([[idx, x] for idx, x in enumerate(xl) if x != 0])\n",
    "\n",
    "    return P_, L, XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a00940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DH_AL(u, ch, B, T, y):\n",
    "    # Initialize variables\n",
    "    N = len(u[0])\n",
    "    print(\"N is \", N)\n",
    "    Nu = [len(ue) for ue in u]\n",
    "    wu = np.array(Nu) / N\n",
    "    \n",
    "    z = np.empty((0, 3))\n",
    "    uz = [ [] for _ in range(N-1)]\n",
    "    u_ = u.copy()\n",
    "    \n",
    "    pl = [None] * len(u)\n",
    "    Aul = [None] * len(u)\n",
    "    Au = np.zeros(len(u) + N)\n",
    "    eu = np.ones(len(u) + N)\n",
    "    Lu = [None] * len(u)\n",
    "    \n",
    "    P = [0]\n",
    "    \n",
    "    for t in range(1, T+1):\n",
    "        for b in range(1, B+1):\n",
    "            # Select v from P\n",
    "            prop = wu[P]\n",
    "            for i, v in enumerate(P):\n",
    "                coeff = 1 if len(P) == 1 else 0 if not u_[v] else 1 - max(pl[v][:, -2])\n",
    "                prop[i] *= coeff\n",
    "            \n",
    "            prob = prop / np.sum(prop)\n",
    "            vi = choices(range(len(prob)), prob)[0]\n",
    "            \n",
    "            # Query label\n",
    "            s = 0\n",
    "            if len(u_[P[vi]]) != 0:\n",
    "                s = int(sample(u_[P[vi]], 1)[0])\n",
    "            else:\n",
    "                continue\n",
    "            for ui in u_:\n",
    "                if s in ui:\n",
    "                    ui.remove(s)\n",
    "            \n",
    "            l = y[s]\n",
    "            \n",
    "            sampled_vec = np.array([s, P[vi], l])\n",
    "            z = np.vstack([z, sampled_vec])\n",
    "            \n",
    "            # Update node counts\n",
    "            u_i = [i for i, ue in enumerate(u) if s in ue]\n",
    "            for uw in u_i:\n",
    "                uz[uw].append((s, l))\n",
    "                nu = len(uz[uw])\n",
    "                \n",
    "                cl, c = zip(*Counter([l for s, l in uz[uw]]).items())\n",
    "                p_l = np.array(c) / nu\n",
    "                \n",
    "                delta = 1 / nu + np.sqrt((p_l * (1 - p_l)) / nu)\n",
    "                lb = np.maximum(p_l - delta, 0)\n",
    "                ub = np.minimum(p_l + delta, 1)\n",
    "                \n",
    "                pl[uw] = np.column_stack((cl, p_l, lb, ub))\n",
    "                Lu[uw] = cl[np.argmax(p_l)]\n",
    "                \n",
    "        # Update admissibilities, error/scores\n",
    "        u_i = [i for i, ple in enumerate(pl) if ple is not None]\n",
    "        for uw in u_i:\n",
    "            beta = 1.5\n",
    "            p_l = pl[uw]\n",
    "            if len(uz[uw]) > 1:\n",
    "                LHS = p_l[:, -2]\n",
    "                RHS = beta * p_l[:, -1] - 1\n",
    "                a_l = LHS[:, None] > RHS\n",
    "                \n",
    "                a_l[np.diag_indices_from(a_l)] = False\n",
    "                idx = np.all(a_l, axis=1)\n",
    "                \n",
    "                Aul[uw] = p_l[idx, 0]\n",
    "                Au[uw] = np.any(idx)\n",
    "                eu[uw] = 1 - np.max(p_l[:, 1]) if np.any(idx) else 1\n",
    "                \n",
    "        eu[Au == 0] = 1\n",
    "        \n",
    "        # Refine pruning & labelling\n",
    "        P, L, XL = prune_label(P, u, Au, Lu, ch, z, N, eu, wu)\n",
    "    \n",
    "    return XL, np.array(z), P, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fe3de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nexample run from https://github.com/labull/EngineeringPatternRecognition/tree/main/matlab/cluster_based_active_learning\\nn = 100;\\nB = 3; % batch size\\nt = n/3; % number of runs\\n\\n% run the DH learner\\n[xl, z] = DH_AL(u, ch, B, t, Y);\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "example run from https://github.com/labull/EngineeringPatternRecognition/tree/main/matlab/cluster_based_active_learning\n",
    "n = 100;\n",
    "B = 3; % batch size\n",
    "t = n/3; % number of runs\n",
    "\n",
    "% run the DH learner\n",
    "[xl, z] = DH_AL(u, ch, B, t, Y);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa508101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader import fetch_signals\n",
    "from preprocess import preprocess\n",
    "from feature_extractor import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# gets X_train, y_train or X_test, y_test (pca and std should be None if train=True)\n",
    "def get_train_test(pca = None, scaler = None, train = True):  \n",
    "    print(\"benchmark 0\")  \n",
    "    signals, labels = fetch_signals('C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\data', train = train)\n",
    "    print(\"benchmark 1\")\n",
    "    preprocessed_sig = np.array([preprocess(signal) for signal in signals])\n",
    "    print(\"benchmark 2\")\n",
    "    print(preprocessed_sig.shape)\n",
    "\n",
    "    # extract the characteristics\n",
    "    X_train_time =  np.apply_along_axis(time_domain_characteristics, axis = 1, arr = preprocessed_sig) \n",
    "    print(\"benchmark 3\")\n",
    "    X_train_freq = np.apply_along_axis(frequency_domain_characteristics, axis = 1, arr = preprocessed_sig) \n",
    "    print(\"benchmark 4\")\n",
    "    Y = np.array(labels) - 1\n",
    "\n",
    "    # unstd stands for unstandardized\n",
    "    X_unstd = np.hstack((X_train_time, X_train_freq))\n",
    "    mask_finite = np.isfinite(X_unstd).all(axis=1)\n",
    "    X_unstd = X_unstd[mask_finite]\n",
    "    Y = Y[mask_finite]\n",
    "\n",
    "    # standardize the data and reduce dimensionality using PCA\n",
    "    if ((scaler  == None) or (pca == None)):\n",
    "        scaler = StandardScaler()\n",
    "        X_std = scaler.fit_transform(X_unstd)\n",
    "        pca = PCA(n_components=0.9, svd_solver = 'full') \n",
    "        X = pca.fit_transform(X_std)\n",
    "\n",
    "        # precompute\n",
    "        np.save('x_train_std_no_pca.npy', X_std)\n",
    "        np.save('x_train_std_pca.npy', X)\n",
    "        np.save('y_train.npy', Y)\n",
    "\n",
    "        return X, Y, pca, scaler\n",
    "    else:\n",
    "        X_std = scaler.transform(X_unstd)\n",
    "        X = pca.transform(X_std)\n",
    "\n",
    "        # precompute\n",
    "        np.save('x_test_std_no_pca.npy', X_std)\n",
    "        np.save('x_test_std_pca.npy', X)\n",
    "        np.save('y_test.npy', Y)\n",
    "\n",
    "        return X, Y, pca, scaler\n",
    "\n",
    "    return None, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3e05c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark 0\n",
      "benchmark 1\n",
      "benchmark 2\n",
      "(2244, 16384)\n",
      "benchmark 3\n",
      "benchmark 4\n",
      "benchmark 0\n",
      "benchmark 1\n",
      "benchmark 2\n",
      "(17536, 16384)\n",
      "benchmark 3\n",
      "benchmark 4\n"
     ]
    }
   ],
   "source": [
    "X, Y, pca, scaler = get_train_test()\n",
    "x_test, y_test, _ , _ = get_train_test(pca = pca, scaler = scaler, train = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f46d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17472, 515)\n",
      "(17472,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61839c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling strategy (makes sure all *different* labels are included, plus whatever is needed to fill the algorithm run budget of t*B)\n",
    "def sample_indices_covering_all_Y_values(Y, N):\n",
    "    unique_values = np.unique(Y)\n",
    "    sampled_indices = []\n",
    "    \n",
    "    if N < len(unique_values):\n",
    "        raise ValueError(f\"N must be greater than or equal to the number of unique values in Y ({len(unique_values)}).\")\n",
    "        \n",
    "    for value in unique_values:\n",
    "        possible_indices = np.where(Y == value)[0]\n",
    "        sampled_index = np.random.choice(possible_indices)\n",
    "        sampled_indices.append(sampled_index)\n",
    "        \n",
    "    # calculate the remaining number of indices needed\n",
    "    remaining = N - len(unique_values)\n",
    "    \n",
    "    # exclude already sampled indices\n",
    "    all_indices = np.arange(len(Y))\n",
    "    remaining_indices = np.setdiff1d(all_indices, sampled_indices)\n",
    "    \n",
    "    # sample additional random indices to meet the required N\n",
    "    additional_indices = np.random.choice(remaining_indices, size=remaining, replace=False)\n",
    "    \n",
    "    # combine original sampled indices with the additional random indices\n",
    "    final_sampled_indices = np.concatenate([sampled_indices, additional_indices])\n",
    "    \n",
    "    return final_sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76d785fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY BUDGET ------ 400\n",
      "N is  2241\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "********** done ****************\n",
      "N is  2241\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "********** done ****************\n",
      "N is  2241\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "********** done ****************\n",
      "N is  2241\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "********** done ****************\n",
      "N is  2241\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "value of Au is \n",
      "0.0\n",
      "***\n",
      "********** done ****************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     y_train \u001b[39m=\u001b[39m xl[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m     44\u001b[0m     \u001b[39m# CLASSIFICATION\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     y_pred \u001b[39m=\u001b[39m predict(x_train, y_train, x_test, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     46\u001b[0m     acc\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39msum(y_pred \u001b[39m==\u001b[39m binarize(y_test)) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y_test))\n\u001b[0;32m     48\u001b[0m e_al\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmean(acc))\n",
      "File \u001b[1;32mc:\\Users\\amroa\\Documents\\thesis\\predict.py:66\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(x_train, y_train, x_test, method)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[0;32m     65\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m     68\u001b[0m \u001b[39m# Save best parameters and score\u001b[39;00m\n\u001b[0;32m     69\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from predict import predict\n",
    "from predict import binarize\n",
    "\n",
    "T = 30  # maximum number of runs (max label budget T*B = 600)\n",
    "B = 20  # batch size\n",
    "reps = 20  # number of repeats for each experiment\n",
    "\n",
    "# For storing error\n",
    "e_rs = np.loadtxt('e_rs.txt', dtype=float)\n",
    "\n",
    "\"\"\"\n",
    "for t in range(1, T + 1):\n",
    "    print(f'\\nQUERY BUDGET ------ {t * B}')\n",
    "    acc = []  # accuracy for each repeat\n",
    "    for r in range(reps):\n",
    "        # define the training-set by a random sample from available data\n",
    "        train_idx = np.random.choice(X.shape[0], t * B, replace=False)\n",
    "        x_train = X[train_idx, :]\n",
    "        y_train = Y[train_idx]\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        y_pred = predict(x_train, y_train, x_test, 1)\n",
    "        acc.append(np.sum(y_pred == binarize(y_test)) / len(y_test))\n",
    "\n",
    "    e_rs.append(1 - np.mean(acc))\n",
    "\"\"\"\n",
    "\n",
    "# CLUSTER BASED ACTIVE LEARNING: the DH learner for an increasing label budget\n",
    "e_al = []\n",
    "for t in range(20, T + 1):\n",
    "    print(f'\\nQUERY BUDGET ------ {t * B}')\n",
    "    acc = []\n",
    "    for r in range(reps):\n",
    "        # INITIAL CLUSTERING\n",
    "        u, ch = h_cluster(X, max_clusters=100)\n",
    "\n",
    "        # HIERARCHICAL SAMPLING FOR ACTIVE LEARNING\n",
    "        xl, z, p, l = DH_AL(u, ch, B, t, Y)\n",
    "        print(\"********** done ****************\")\n",
    "        train_idx = xl[:, 0].astype(int)  # convert to int for indexing\n",
    "        x_train = X[train_idx, :]\n",
    "        y_train = xl[:, 1]\n",
    "\n",
    "        # CLASSIFICATION\n",
    "        y_pred = predict(x_train, y_train, x_test, 1)\n",
    "        acc.append(np.sum(y_pred == binarize(y_test)) / len(y_test))\n",
    "\n",
    "    e_al.append(1 - np.mean(acc))\n",
    "\n",
    "# PLOT\n",
    "plt.figure(3)\n",
    "plt.plot(np.arange(B, B * T + 1, B), e_rs, '--')\n",
    "plt.plot(np.arange(B, B * T + 1, B), e_al, '--')\n",
    "plt.xlabel('label budget (n)')\n",
    "plt.ylabel('classification error (e)')\n",
    "plt.legend(['passive learning', 'active learning'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "569ada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('e_rs.txt', 'w') as f:\n",
    "    for item in e_rs:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad55a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('e_al.txt', 'w') as f:\n",
    "    for item in e_al:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfd141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
