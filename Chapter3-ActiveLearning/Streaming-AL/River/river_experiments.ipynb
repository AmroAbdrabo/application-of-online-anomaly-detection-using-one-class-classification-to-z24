{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental learning with River"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import active\n",
    "from river import datasets\n",
    "import numpy as np\n",
    "from river import feature_extraction\n",
    "from river import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from river import metrics\n",
    "import sys\n",
    "sys.path.append('.\\\\..\\\\..\\\\..\\\\Chapter2-Z24-dataset')\n",
    "from predict import binarize\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\X_train.npy\")\n",
    "labels_train = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\labels_train.npy\")\n",
    "X_test = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\X_test.npy\")\n",
    "labels_test = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\labels_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((544, 906), (68, 906))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, \\\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amroa\\Documents\\thesis\\Chapter3-ActiveLearning\\Streaming-AL\\River\\river_experiments.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Streaming-AL/River/river_experiments.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m log_reg \u001b[39m=\u001b[39m LogisticRegression(C \u001b[39m=\u001b[39m \u001b[39m12.5\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Streaming-AL/River/river_experiments.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_train \u001b[39m=\u001b[39m binarize(labels_train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Streaming-AL/River/river_experiments.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m log_reg\u001b[39m.\u001b[39;49mfit(X_train, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Streaming-AL/River/river_experiments.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sorted_feature_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(log_reg\u001b[39m.\u001b[39mcoef_[\u001b[39m0\u001b[39m])[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m# get the indices of the most important features in descending order\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1167\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1184\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1185\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[0;32m   1186\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32mc:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1245\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1234\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1235\u001b[0m             (\n\u001b[0;32m   1236\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1241\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   1242\u001b[0m         )\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[1;32m-> 1245\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1246\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[0;32m   1247\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "features_to_keep = 79\n",
    "neighbors = 41\n",
    "\n",
    "y_train = binarize(labels_train)\n",
    "log_reg = LogisticRegression(C = 12.5) \n",
    "y_train = binarize(labels_train)\n",
    "log_reg.fit(X_train, y_train)\n",
    "sorted_feature_indices = np.argsort(log_reg.coef_[0])[::-1] # get the indices of the most important features in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction (to stabilize KNN and avoid dimensionality curse)\n",
    "indices = sorted_feature_indices[:features_to_keep]\n",
    "X_train_top_feats = X_train[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.79%\n",
      "Number of samples used for learning: 94\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets as skdatasets  # Import to avoid naming conflicts\n",
    "from river import active\n",
    "from river import datasets\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "\n",
    "# Prepare the data in a format compatible with river\n",
    "X_river = [\n",
    "    {f\"feature_{i}\": value for i, value in enumerate(sample)}\n",
    "    for sample in X_train_top_feats\n",
    "]\n",
    "y_river = y_train.tolist()  # Converting to list for easier iteration\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "base_model = linear_model.LogisticRegression()\n",
    "model = active.EntropySampler(base_model, discount_factor=20, seed=42)\n",
    "\n",
    "n_samples_used = 0\n",
    "\n",
    "# Process each sample in the dataset\n",
    "for i in range(len(X_river)):\n",
    "    x = X_river[i]\n",
    "    y_true = y_river[i]\n",
    "    y_pred, ask = model.predict_one(x)  # x is now a dictionary\n",
    "    metric = metric.update(y_true, y_pred)\n",
    "\n",
    "    if ask:\n",
    "        n_samples_used += 1\n",
    "        model = model.learn_one(x, y_true)  # Learn from the true label\n",
    "\n",
    "print(metric)\n",
    "print(f\"Number of samples used for learning: {n_samples_used}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_top_f = X_test[:, indices]\n",
    "y_test = binarize(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.06%\n"
     ]
    }
   ],
   "source": [
    "X_river_test = [\n",
    "    {f\"feature_{i}\": value for i, value in enumerate(sample)}\n",
    "    for sample in X_test_top_f\n",
    "]\n",
    "y_river_test = y_test.tolist()  # Converting to list for easier iteration\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "# Process each sample in the dataset\n",
    "for i in range(len(X_river_test)):\n",
    "    x = X_river_test[i]\n",
    "    y_true = y_river_test[i]\n",
    "    y_pred, _ = model.predict_one(x)  # x is now a dictionary\n",
    "    metric = metric.update(y_true, y_pred)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplementary: Example usage of RiverML on iris from Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import datasets as skdatasets  # Import to avoid naming conflicts\\nfrom river import active\\nfrom river import datasets\\nfrom river import linear_model\\nfrom river import metrics\\n\\n# Load the iris dataset from sklearn\\niris_sk = skdatasets.load_iris()\\n\\n# Prepare the data in a format compatible with river\\nX_river = [\\n    {f\"feature_{i}\": value for i, value in enumerate(sample)}\\n    for sample in iris_sk[\\'data\\']\\n]\\ny_river = iris_sk[\\'target\\'].tolist()  # Converting to list for easier iteration\\n\\nmetric = metrics.Accuracy()\\nbase_model = linear_model.LogisticRegression()\\nmodel = active.EntropySampler(base_model, seed=42)\\n\\nn_samples_used = 0\\n\\n# Process each sample in the dataset\\nfor i in range(len(X_river)):\\n    x = X_river[i]\\n    y_true = y_river[i]\\n    y_pred, ask = model.predict_one(x)  # x is now a dictionary\\n    metric = metric.update(y_true, y_pred)\\n\\n    if ask:\\n        n_samples_used += 1\\n        model = model.learn_one(x, y_true)  # Learn from the true label\\n\\nprint(metric)\\nprint(f\"Number of samples used for learning: {n_samples_used}\")\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn import datasets as skdatasets  # Import to avoid naming conflicts\n",
    "from river import active\n",
    "from river import datasets\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "\n",
    "# Load the iris dataset from sklearn\n",
    "iris_sk = skdatasets.load_iris()\n",
    "\n",
    "# Prepare the data in a format compatible with river\n",
    "X_river = [\n",
    "    {f\"feature_{i}\": value for i, value in enumerate(sample)}\n",
    "    for sample in iris_sk['data']\n",
    "]\n",
    "y_river = iris_sk['target'].tolist()  # Converting to list for easier iteration\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "base_model = linear_model.LogisticRegression()\n",
    "model = active.EntropySampler(base_model, seed=42)\n",
    "\n",
    "n_samples_used = 0\n",
    "\n",
    "# Process each sample in the dataset\n",
    "for i in range(len(X_river)):\n",
    "    x = X_river[i]\n",
    "    y_true = y_river[i]\n",
    "    y_pred, ask = model.predict_one(x)  # x is now a dictionary\n",
    "    metric = metric.update(y_true, y_pred)\n",
    "\n",
    "    if ask:\n",
    "        n_samples_used += 1\n",
    "        model = model.learn_one(x, y_true)  # Learn from the true label\n",
    "\n",
    "print(metric)\n",
    "print(f\"Number of samples used for learning: {n_samples_used}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
