{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCC solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys\n",
    "sys.path.append('.\\\\..\\\\..\\\\Chapter2-Z24-dataset')\n",
    "from predict import binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\X_train.npy\")\n",
    "labels_train = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\labels_train.npy\")\n",
    "X_test = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\X_test.npy\")\n",
    "labels_test = np.load(\"C:\\\\Users\\\\amroa\\\\Documents\\\\thesis\\\\labels_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "# extract the relevant file indices for training and testing separately\n",
    "test_indices=  np.array([])\n",
    "train_indices=  np.array([])\n",
    "\n",
    "for i in range(17):\n",
    "    random.seed(i)\n",
    "    nbr_splits = 36 #  recall 612/17 = 36, where 612 = total nbr of epochs, 17 is nbr of scenarios\n",
    "    test_split = 4\n",
    "    test_indices_temp = np.array(random.sample(range(nbr_splits), test_split))\n",
    "    test_indices = np.append(test_indices, i*nbr_splits + test_indices_temp)\n",
    "    train_indices = np.append(train_indices,  i*nbr_splits+ np.array([x for x in range(nbr_splits) if x not in test_indices_temp]))\n",
    "\n",
    "print(len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ctr = 0\n",
    "test_ctr = 0\n",
    "data_all = []\n",
    "labels_all = []\n",
    "for i in range(612):\n",
    "    if i in train_indices:\n",
    "        data_all.append(X_train[train_ctr])\n",
    "        labels_all.append(labels_train[train_ctr])\n",
    "        train_ctr += 1\n",
    "    elif i in test_indices:\n",
    "        data_all.append(X_test[test_ctr])\n",
    "        labels_all.append(labels_test[test_ctr])\n",
    "        test_ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = binarize(labels_all) # is the labels of all instances \n",
    "data_all = np.array(data_all) # are all the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters and data\n",
    "features_to_keep = 79\n",
    "neighbors = 41\n",
    "y_train = binarize(labels_all)\n",
    "log_reg = LogisticRegression(C = 12.5) \n",
    "y_train = binarize(labels_train)\n",
    "log_reg.fit(X_train, y_train)\n",
    "sorted_feature_indices = np.argsort(log_reg.coef_[0])[::-1] # get the indices of the most important features in descending order\n",
    "indices = sorted_feature_indices[:features_to_keep]\n",
    "data_all_top = data_all[:, indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((288, 79), (324, 79))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_data = data_all_top[y_all == 0]\n",
    "unhealthy_data = data_all_top[y_all == 1]\n",
    "healthy_data.shape, unhealthy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((216, 79), (324, 79), (72, 79))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "test_size = 0.25\n",
    "healthy_train, healthy_test = train_test_split(healthy_data, test_size=test_size, stratify=np.repeat(np.arange(8), 36), random_state=42)\n",
    "dam_test = unhealthy_data\n",
    "healthy_train.shape, dam_test.shape, healthy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 34)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = healthy_train\n",
    "train_labels = np.full(train_data.shape[0], 1).astype(int)\n",
    "\n",
    "pca = PCA(n_components=0.9)\n",
    "sc = StandardScaler()\n",
    "\n",
    "train_data = sc.fit_transform(train_data)\n",
    "train_data = pca.fit_transform(train_data)\n",
    "\n",
    "data_all_sc = sc.transform(data_all_top)\n",
    "data_all_pca = pca.transform(data_all_sc)\n",
    "\n",
    "data_all_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_occ_svm = {\n",
    "    'nu': [0.01, 0.1, 0.2], \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'tol': [1e-3, 1e-2, 1e-1]\n",
    "} # rbf by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plt_distr(predictions):\n",
    "    global data_all_pca\n",
    "    sns.reset_defaults()\n",
    "    sns.reset_orig()\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    nbr_epochs = data_all_pca.shape[0]\n",
    "\n",
    "    preds = np.array(predictions).reshape(-1, 1)\n",
    "    for i in range(0, nbr_epochs+1, 32):\n",
    "        plt.axvline(x=i, color='red', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    obs = np.arange(len(preds))\n",
    "    plt.bar(obs, preds)\n",
    "    plt.ylim(0, 2)\n",
    "\n",
    "    plt.xlabel('Epoch (16,384 samples or 2.7 min per epoch)')\n",
    "    plt.ylabel('Entropies')\n",
    "    plt.title(f'Changepoints detected')\n",
    "    text_pos = [i+16 for i in range(0, nbr_epochs, 32)]\n",
    "    for idx, pos in enumerate(text_pos):\n",
    "        plt.text(pos, 2, f'DS {idx+1}', ha='center', color='red')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = data_all_top.shape[0]\n",
    "grace = 20\n",
    "start = 6\n",
    "predictions = [0]*start\n",
    "for i in range(start, cnt):\n",
    "    model = GridSearchCV(OneClassSVM(), param_grid_occ_svm, scoring = 'accuracy', cv = 5)\n",
    "    model.fit(data_all_pca[:i], np.zeros(i))\n",
    "    y_pred = 1 if i < grace else model.predict([data_all_pca[i]])\n",
    "    predictions.append(max(-y_pred, 0)) # anomaly maps to 1 then 1 and normals maps to -1 to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef tune_parameters(healthy_train, combined_validation, len_val_healthy, len_test_healthy, param_grid):\\n    best_score = 0\\n    best_params = None\\n    \\n    for nu in param_grid['nu']:\\n        for gamma in param_grid['gamma']:\\n            model = OneClassSVM(nu=nu, gamma=gamma)\\n            score, _ = evaluate_model(model, healthy_train, combined_validation, None, len_val_healthy, len_test_healthy, tune=True)\\n            \\n            if score > best_score:\\n                best_score = score\\n                best_params = {'nu': nu, 'gamma': gamma}\\n    \\n    return best_params\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def tune_parameters(healthy_train, combined_validation, len_val_healthy, len_test_healthy, param_grid):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for nu in param_grid['nu']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            model = OneClassSVM(nu=nu, gamma=gamma)\n",
    "            score, _ = evaluate_model(model, healthy_train, combined_validation, None, len_val_healthy, len_test_healthy, tune=True)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'nu': nu, 'gamma': gamma}\n",
    "    \n",
    "    return best_params\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_imblearn_pipeline\n",
    "\n",
    "def split_data(healthy_data, unhealthy_data, test_size, validation_size):\n",
    "    # Split healthy data into training, validation, and testing\n",
    "    healthy_train, temp_healthy = train_test_split(healthy_data, test_size=test_size + validation_size, random_state=42)\n",
    "    healthy_validation, healthy_test = train_test_split(temp_healthy, test_size=test_size / (test_size + validation_size), random_state=42)\n",
    "\n",
    "    # Split unhealthy data into validation and testing\n",
    "    unhealthy_validation, unhealthy_test = train_test_split(unhealthy_data, test_size=test_size/ (test_size + validation_size), random_state=42)\n",
    "    print(\"solutions\")\n",
    "    #print(unhealthy_validation.shape)\n",
    "    print(unhealthy_test.shape)\n",
    "    print(healthy_test.shape)\n",
    "    print(healthy_train.shape)\n",
    "\n",
    "    # Combine healthy and unhealthy data for validation and testing\n",
    "    combined_validation = np.vstack([healthy_validation, unhealthy_validation])\n",
    "    print(combined_validation.shape)\n",
    "    combined_test = np.vstack([healthy_test, unhealthy_test])\n",
    "\n",
    "    len_val_healthy, len_test_healthy = healthy_validation.shape[0], healthy_test.shape[0]\n",
    "\n",
    "    return healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def tune_parameters(healthy_train, combined_validation, len_val_healthy, param_grid):\n",
    "    # Pipeline with preprocessing and model\n",
    "    pipeline = make_pipeline(StandardScaler(),  PCA(n_components=31), OneClassSVM(nu = 0.01))\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "    # Prepare validation labels\n",
    "    validation_labels = np.concatenate([np.zeros(len_val_healthy), np.ones(combined_validation.shape[0]-len_val_healthy)])\n",
    "    unhealthy_train = combined_validation[len_val_healthy:, ]\n",
    "    combine_train = np.vstack((healthy_train, unhealthy_train))\n",
    "    labels = np.hstack((np.ones(healthy_train.shape[0]),-np.ones(unhealthy_train.shape[0]) ))\n",
    "    \n",
    "    # Perform grid search\n",
    "    pipeline.fit(healthy_train, 0*np.ones(healthy_train.shape[0]))\n",
    "    pcd = PCA(n_components=10)\n",
    "    pcd.fit(healthy_train)\n",
    "    print(pcd.n_components_)\n",
    "    print(\"ds\")\n",
    "    #grid_search.fit(combine_train, labels)\n",
    "    #best_model = grid_search.best_estimator_\n",
    "    #print(grid_search.best_params_)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy, tune=False):\n",
    "    model.fit(healthy_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    preds_validation = model.predict(combined_validation)\n",
    "    preds_validation = (preds_validation == -1).astype(int)\n",
    "    validation_labels = np.concatenate([np.zeros(len_val_healthy), np.ones(combined_validation.shape[0]-len_val_healthy)])\n",
    "    \n",
    "    # Evaluate model on validation set\n",
    "    if tune:\n",
    "        score = accuracy_score(validation_labels, preds_validation)\n",
    "        return score, model\n",
    "    else:\n",
    "        print(\"Validation Results:\")\n",
    "        print(classification_report(validation_labels, preds_validation))\n",
    "        print(\"Accuracy:\", accuracy_score(validation_labels, preds_validation))\n",
    "    \n",
    "    # If test data is provided, evaluate on test set\n",
    "    if combined_test is not None:\n",
    "        preds_test = model.predict(combined_test)\n",
    "        preds_test = (preds_test == -1).astype(int)\n",
    "        test_labels = np.concatenate([np.zeros(len_test_healthy), np.ones(combined_test.shape[0]-len_test_healthy)])\n",
    "        print(\"Test Results:\")\n",
    "        print(classification_report(test_labels, preds_test))\n",
    "        print(\"Accuracy:\", accuracy_score(test_labels, preds_test))\n",
    "        return None, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amroa\\Documents\\thesis\\Chapter3-ActiveLearning\\Irmos-Working-Soln\\solution.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_size \u001b[39m=\u001b[39m \u001b[39m0.15\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m validation_size \u001b[39m=\u001b[39m \u001b[39m0.15\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy \u001b[39m=\u001b[39m split_data(healthy_data, unhealthy_data, test_size, validation_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m#'oneclasssvm__nu': [0.01, 0.05, 0.1, 0.2, 0.3],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moneclasssvm__nu\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moneclasssvm__gamma\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.001\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Consider adding more parameters here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m param_grid_lof \u001b[39m=\u001b[39m  {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocaloutlierfactor__n_neighbors\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m,  \u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m40\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocaloutlierfactor__contamination\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.2\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocaloutlierfactor__leaf_size\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m40\u001b[39m, \u001b[39m50\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlocaloutlierfactor__metric\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mminkowski\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmanhattan\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amroa/Documents/thesis/Chapter3-ActiveLearning/Irmos-Working-Soln/solution.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_size = 0.15\n",
    "validation_size = 0.15\n",
    "healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy = split_data(healthy_data, unhealthy_data, test_size, validation_size)\n",
    "\n",
    "param_grid = {\n",
    "    #'oneclasssvm__nu': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'oneclasssvm__nu': [0.01], \n",
    "    'oneclasssvm__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    # Consider adding more parameters here\n",
    "}\n",
    "\n",
    "param_grid_lof =  {\n",
    "    'localoutlierfactor__n_neighbors': [2,  10, 20, 40],\n",
    "    'localoutlierfactor__contamination': [0.01, 0.05, 0.1, 0.2],\n",
    "    'localoutlierfactor__leaf_size': [20, 30, 40, 50],\n",
    "    'localoutlierfactor__metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "}\n",
    "param_grid = {}\n",
    "\n",
    "best_model = tune_parameters(healthy_train, combined_validation, len_val_healthy, param_grid)\n",
    "_, fitted_model = evaluate_model(best_model, healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        43\n",
      "         1.0       0.86      1.00      0.93       275\n",
      "\n",
      "    accuracy                           0.86       318\n",
      "   macro avg       0.43      0.50      0.46       318\n",
      "weighted avg       0.75      0.86      0.80       318\n",
      "\n",
      "Accuracy: 0.8647798742138365\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        44\n",
      "         1.0       0.53      1.00      0.69        49\n",
      "\n",
      "    accuracy                           0.53        93\n",
      "   macro avg       0.26      0.50      0.35        93\n",
      "weighted avg       0.28      0.53      0.36        93\n",
      "\n",
      "Accuracy: 0.5268817204301075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Rest of your code remains the same\n",
    "\n",
    "def tune_parameters(healthy_train, combined_validation, len_val_healthy):\n",
    "    # Manual tuning of GMM parameters (or use a different approach)\n",
    "    best_gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
    "\n",
    "    ### added part \n",
    "    validation_labels = np.concatenate([np.zeros(len_val_healthy), np.ones(combined_validation.shape[0]-len_val_healthy)])\n",
    "    unhealthy_train = combined_validation[len_val_healthy:, ]\n",
    "    combine_train = np.vstack((healthy_train, unhealthy_train))\n",
    "    labels = np.hstack((np.ones(healthy_train.shape[0]),-np.ones(unhealthy_train.shape[0]) ))\n",
    "    \n",
    "    # Perform grid search\n",
    "    #grid_search.fit(healthy_train, np.zeros(healthy_train.shape[0]))\n",
    "    #grid_search.fit(combine_train, labels)\n",
    "\n",
    "\n",
    "    #### end added part \n",
    "    best_gmm.fit(combine_train)\n",
    "\n",
    "    return best_gmm\n",
    "\n",
    "def evaluate_model(model, healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy, threshold=-10):\n",
    "    model.fit(healthy_train)\n",
    "    \n",
    "    # Compute log likelihood of validation data\n",
    "    log_likelihood_validation = model.score_samples(combined_validation)\n",
    "    preds_validation = (log_likelihood_validation < threshold).astype(int)\n",
    "    validation_labels = np.concatenate([np.zeros(len_val_healthy), np.ones(combined_validation.shape[0] - len_val_healthy)])\n",
    "    \n",
    "    # Evaluate model on validation set\n",
    "    print(\"Validation Results:\")\n",
    "    print(classification_report(validation_labels, preds_validation))\n",
    "    print(\"Accuracy:\", accuracy_score(validation_labels, preds_validation))\n",
    "    \n",
    "    # Evaluate on test set, if provided\n",
    "    if combined_test is not None:\n",
    "        log_likelihood_test = model.score_samples(combined_test)\n",
    "        preds_test = (log_likelihood_test < threshold).astype(int)\n",
    "        test_labels = np.concatenate([np.zeros(len_test_healthy), np.ones(combined_test.shape[0] - len_test_healthy)])\n",
    "        print(\"Test Results:\")\n",
    "        print(classification_report(test_labels, preds_test))\n",
    "        print(\"Accuracy:\", accuracy_score(test_labels, preds_test))\n",
    "\n",
    "healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy = split_data(healthy_data, unhealthy_data, test_size, validation_size)\n",
    "\n",
    "best_model = tune_parameters(healthy_train, combined_validation, len_val_healthy)\n",
    "evaluate_model(best_model, healthy_train, combined_validation, combined_test, len_val_healthy, len_test_healthy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into healthy and unhealthy instances\n",
    "healthy_data = data_all_top[y_all == 0]\n",
    "unhealthy_data = data_all_top[y_all == 1]\n",
    "\n",
    "unhealthy_validation, unhealthy_test = train_test_split(\n",
    "    unhealthy_data, test_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# crwate models\n",
    "svm_model = OneClassSVM(kernel='linear', gamma='auto')\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, novelty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((288, 79), (324, 79))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_data.shape, unhealthy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating One-Class SVM...\n",
      "Using 14 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 43 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       0.50      0.50      0.50       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 0.9961538461538462\n",
      "Using 72 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 100 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 129 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 158 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 187 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 216 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 244 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n",
      "Using 273 instances\n",
      "Validation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Accuracy: 1.0\n",
      "Test Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\amroa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate all\n",
    "print(\"Evaluating One-Class SVM...\")\n",
    "for el in np.arange(0.05, 1.0, 0.1):\n",
    "    evaluate_model(svm_model, healthy_data, unhealthy_validation, unhealthy_test, el)\n",
    "\n",
    "#print(\"\\nEvaluating Isolation Forest...\")\n",
    "#evaluate_model(isolation_forest, healthy_data, unhealthy_validation, unhealthy_test)\n",
    "\n",
    "#print(\"\\nEvaluating Local Outlier Factor...\")\n",
    "#evaluate_model(lof_model, healthy_data, unhealthy_validation, unhealthy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "       611])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.hstack((test_indices, train_indices))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
